---
title: "Urnmodel Rmd"
author: "Fabio Votta"
date: "`r Sys.Date()`"
output: html_document
---

This script is about:

A very first approach at Bayesian statistics

## Packages and Folders

```{r}
# Install these packages if you don't have them yet
# if (!require("pacman")) install.packages("pacman")
# devtools::install_github("favstats/tidytemplate")

pacman::p_load(tidyverse, tidytemplate)

# Creates folders
# tidytemplate::data_dir()
# tidytemplate::images_dir()

# I don't like the default theme in ggplot.
theme_set(theme_minimal())

# options(scipen = 999)

knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
```


Assume we have N (by default 100) marbles in an urn.
Some are red, the remainder are black.
We sample n marbles *with* replacement.
We find m are red, so n-m are black.
Return the likelihood function for
each of the (N+1) possible numbers of
red marbles in the urn.

```{r, fig.width=10}
binomial_likelihood <- function(m, n, N){
  
  x <- seq(0, N)
  p <- x/N
  
  tibble(x = x,
         y = p^m * (1-p)^(n-m)
  )
}

plot_binomial_likelihood <- function(m, n, N){
  title <- sprintf('Likelihood. %d red having sampled %d with replacement from urn with %d', m, n, N)
  binomial_likelihood(m, n, N) %>% fplot('likelihood', title)
}

fplot <- function(Df, ylabel=NULL, title=NULL){
  ggplot(Df,
         aes(x = x, y = y)) + 
    ylab(ylabel) +
    geom_bar(stat='identity', width = 0.5) +
    ggtitle(title)
}

```

```{r}
plot_binomial_likelihood(n = 25, m = 8, N = 100)
```

```{r}
binomial_likelihood(n = 25, m = 8, N = 100) %>% 
  mutate(mean = mean(y)) %>% 
  mutate(sd = sd(y)) %>% 
  mutate(maxval = max(y)) %>% 
  mutate(top = ifelse(maxval == y, x, 0)) %>% 
  mutate(band = ifelse(y > maxval - sd, x, 0)) %>% 
  fplot('likelihood', 'Likelihood. 8 red having sampled 25 with replacement from urn with 50') +
  geom_vline(aes(xintercept = top)) +
  # geom_vline(aes(xintercept = band)) +
  geom_col(aes(fill = band)) +
  scale_fill_viridis_c()
```


Assume we have N (by default 100) marbles in an urn.
Some are red, the remainder are black.
We sample n marbles *without* replacement.
We find m are red, so n-m are black.
Return the likelihood function for
each of the (N+1) possible numbers of
red marbles in the urn.

```{r, data}

hypergeometric_likelihood <- function(m, n, N){
  
  tibble(x = seq(0, N),
         y = dhyper(m, x, N-x, n)
  )
}

plot_hypergeometric_likelihood <- function(m, n, N){
  title <- sprintf('Likelihood. %d red having sampled %d without replacement from urn with %d', m, n, N)
  hypergeometric_likelihood(m, n, N) %>% fplot('likelihood', title)
}

plot_hypergeometric_likelihood(12, 25, 100)
```

### Uniform Prior

```{r}
uniform_prior <- function(N){
  tibble(
    x = seq(0, N),
    y = 1/(N+1)
  )
}

uniform_prior(100) %>% 
  ggplot(aes(x, y)) + 
  geom_col()
```

### Pseudo Normal Prior

```{r}
pseudo_normal_prior <- function(N, mean, sd){
  x <- seq(0, N)
  y <- dnorm(x, mean=mean, sd=sd)
  tibble(
    x = x,
    y = y/sum(y)
  )
}

pseudo_normal_prior(100, 75, 10) %>% 
  ggplot(aes(x, y)) +
  geom_col()
```

### Make Posterior

```{r, fig.width=10, fig.height=6}
make_posterior <- function(likelihood, prior){
  
  # Do some checking
  stopifnot(all_equal(likelihood$x, prior$x))
  
  posterior <- likelihood$y * prior$y
  prior %>% 
    mutate(y = posterior/sum(posterior))
}

likelihood_function <- hypergeometric_likelihood(8, 25, 100)
prior_function <- pseudo_normal_prior(100, 75, 10)

plot_together <- function(likelihood_function,
                          prior_function) {
  make_posterior(likelihood_function, prior_function) %>% 
    mutate(type = "posterior") %>% 
    bind_rows(likelihood_function %>% mutate(type = "likelihood")) %>% 
    bind_rows(prior_function %>% mutate(type = "prior")) %>% 
    ggplot(aes(x, y, fill = type)) +
    geom_col(alpha = 0.55, position = position_dodge(), width = 4) +
    scale_fill_viridis_d()  
}

plot_together(likelihood_function, prior_function)
```


#### Crazy Prior

```{r}
crazy_prior <- function(N, K=5){
  y <- sample(seq(K), size = N+1, replace = T)
  tibble(x = seq(0, N),
         y = y/sum(y)
  )
}




likelihood_function <- hypergeometric_likelihood(8, 25, 100)
prior_function <- crazy_prior(100, K = 100)

make_posterior(likelihood_function, prior_function) %>% 
  mutate(type = "posterior") %>% 
  bind_rows(likelihood_function %>% mutate(type = "likelihood")) %>% 
  bind_rows(prior_function %>% mutate(type = "crazyprior")) %>% 
  ggplot(aes(x, y, fill = type), colour = "black") +
  geom_col(alpha = 0.45, position = position_dodge(), width = 4) +
  scale_fill_viridis_d() 
```

### Bernoulli Likelihood

```{r}
source("code/bernoulli.likelihood.R")
```

### Beta Scripts

```{r}
source("code/beta.distribution.R")
source("code/beta.hpd.R")
```

```{r}
beta.plot(10, 10)
```

```{r}
beta.plot.hpd(10, 10)
```

```{r}
beta.plot.hpd(10 + 139, 10 + 111)
```

### Rstan

```{r}
library(rstan)

rstan_options(auto_write=TRUE)

Df <- list(n = 250, 
           m = 139,
           alpha = 1,
           beta = 1)

M <- stan(file = 'code/coin.stan', 
          chains = 4,
          warmup = 1000,
          iter = 2000,
          cores = 2,
          data = Df)

# summary of model
summary(M)

# bettter summary
print(M, pars=c('theta'),
      probs = c(0.025, 0.1, 0.5, 0.9, 0.975)
)


# posterior mean of theta
posterior_mean <- summary(M)$summary[1, 1]

# interval plot
plot(M)

# histogram
stan_hist(M, bins=25)

# density
stan_dens(M)

# Another density plot
plot(M, 
     show_density = TRUE, 
     ci_level = 0.95, 
     color = 'red',
     fill_color = "purple")

# trace plot
traceplot(M, inc_warmup = TRUE)

# Extract samples
samples <- extract(M)$theta

```

### BRMS Demo

```{r}
library(brms)
library(dplyr)
library(readr)
library(ggplot2)

Df <- read_csv("data/diagram.csv") %>% 
  select(-X1)

ggplot(Df,
       aes(x = group, y = descript, col = group)
) + geom_boxplot()


M_null <- brm(descript ~ 1, Df, save_all_pars = T)
summary(M_null)
plot(M_null)

M <- brm(descript ~ group, Df, save_all_pars = T)
summary(M)

plot(M)

# Out of sample predictive performance 
waic(M_null, M)
loo(M_null, M)

# Bayes factor of M_null v M
bayes_factor(M, M_null)

# Directional hypothesis testing
hypothesis(M, "groupPicture < groupSegmentedDiagram")
hypothesis(M, "groupPicture > groupText")
hypothesis(M, "groupSegmentedDiagram + Intercept > Intercept")
hypothesis(M, "groupSegmentedDiagram + Intercept > Intercept + groupText + groupPicture")
hypothesis(M, "groupSegmentedDiagram  > groupText + groupPicture")


# get the old priors
# it has no defined priors for the b coefficients (slopes)
# so Stan probably defaults to a uniform prior which produces problems 
# with Bayes factor calculations
prior_summary(M)

# change the priors for the model
# some new very weakly informative priors
newpriors.i <- c(prior_string("student_t(3, 18, 10)", class = "Intercept"),
                 prior_string("student_t(3, 0, 10)", class = "sigma"))

newpriors <- c(prior_string("student_t(3, 0, 10)", class = "b"),
                 prior_string("student_t(3, 18, 10)", class = "Intercept"),
                 prior_string("student_t(3, 0, 10)", class = "sigma"))

newpriors.cm <- c(prior_string("student_t(3, 18, 10)", class = "b"),
               prior_string("student_t(3, 0, 10)", class = "sigma"))
                 
M_null.new <- brm(descript ~ 1, Df, save_all_pars = T, prior = newpriors.i)
summary(M_null.new)
plot(M_null.new)

M.new <- brm(descript ~ group, Df, save_all_pars = T, prior = newpriors)
summary(M.new)
plot(M.new)


# now the Bayes factor is not dissimilar 
waic(M_null.new, M.new)
loo(M_null.new, M.new)
bayes_factor(M_null.new, M.new)



# refit the model with a cell means parameterization
# one of the workshop attendees was this fitting this version of the model
# here there is no intercept and the four cell means are estimated as parameters

M.cell <- brm(descript ~ 0 + group, Df, save_all_pars = T, prior=newpriors.cm)
summary(M.cell)
plot(M.cell)

waic(M.cell, M_null.new)
bayes_factor(M.cell, M_null.new)






                 
```

```{r}
library (wordcloud2)
letterCloud()

```

